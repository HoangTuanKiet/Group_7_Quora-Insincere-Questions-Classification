{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Objective**: \n",
    "\n",
    "Simple approach to build a sincere/insincere classifier for Quora questions.\n",
    "\n",
    "- **Data preprocssing**\n",
    "   \n",
    "   Data Exploration: few statistics about the questions text.\n",
    "   \n",
    "   Building the vocabulary and then tokenizing the questions text.\n",
    "    \n",
    "\n",
    "\n",
    "- **Modelisation**\n",
    "    \n",
    "    Straight-forward Many-to-One approach with a LSTM layer. LSTM are particular RNN that prevent vanishing gradient and helps the model to better 'remember' when dealing with long sequences. 
    "    \n",
    "    We fed the model with embedded words. In this notebook we will use our own embeddings (trained on the top of the model) rather than the pre-trained ones given as auxiliary inputs. Despite an additional training step for our model, the idea here is that our embedding will be more dedicated to the specific task we are doing than a pre-trained embedding coming from Wikipedia pages.\n",
    "    \n",
    "    \n",
    "- **Predictions and submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam  \n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will explore and make some preprocessing on the data to feed our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (375806, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "3  000042bf85aa498cd78e  ...        0\n",
       "4  0000455dfa3e01eae3af  ...        0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "print(\"Train shape : \", train.shape)\n",
    "print(\"Test shape : \", test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.306122e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.187018e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.409197e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target\n",
       "count  1.306122e+06\n",
       "mean   6.187018e-02\n",
       "std    2.409197e-01\n",
       "min    0.000000e+00\n",
       "25%    0.000000e+00\n",
       "50%    0.000000e+00\n",
       "75%    0.000000e+00\n",
       "max    1.000000e+00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the classes distribution.\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have unbalanced classes: 6.2% insincere vs 93.8% sincere questions. Hence, the performance metrics can't be the accuracy, we will look at the F1-score (as mentioned in the challenge description)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min questions lenght: 1\n",
      "Max questions lenght: 134\n",
      "Mean questions lenght: 12.803609463740754\n",
      "Standard deviation questions lenght: 7.052434330971179\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['lenght_sentence'] = train['question_text'].apply(lambda x: len(x.split()))\n",
    "print('Min questions lenght:', np.min(train['lenght_sentence'] ))\n",
    "print('Max questions lenght:', np.max(train['lenght_sentence'] ))\n",
    "print('Mean questions lenght:', np.mean(train['lenght_sentence'] ))\n",
    "print('Standard deviation questions lenght:', np.std(train['lenght_sentence'] ))\n",
    "\n",
    "\n",
    "# Plot the distribution of the lenght of the questions\n",
    "plt.hist(train['lenght_sentence'], 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min questions lenght: 1\n",
      "Max questions lenght: 87\n",
      "Mean questions lenght: 12.81084123191221\n",
      "Standard deviation questions lenght: 7.04485108423754\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test['lenght_sentence'] = test['question_text'].apply(lambda x: len(x.split()))\n",
    "print('Min questions lenght:', np.min(test['lenght_sentence'] ))\n",
    "print('Max questions lenght:', np.max(test['lenght_sentence'] ))\n",
    "print('Mean questions lenght:', np.mean(test['lenght_sentence'] ))\n",
    "print('Standard deviation questions lenght:', np.std(test['lenght_sentence'] ))\n",
    "\n",
    "# Plot the distribution of the lenght of the questions\n",
    "plt.hist(test['lenght_sentence'], 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the sentences are not of the same size, and we need them to be all at the same format to feed them to our model. \n",
    "\n",
    "To cope with this issue we will truncate too long sentences and use a 0 padding for the short sentences. Thanks to the statistics we have extracted we will use a max_sentence_length = 20 (mean+stddev)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build a vocabulary of all the unique words in the Train and Test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>lenght_sentence</th>\n",
       "      <th>question_text_truncated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>how did quebec nationalists see their province...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>do you have an adopted dog, how would you enco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>why does velocity affect time? does velocity a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>how did otto von guericke used the magdeburg h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>can i convert montra helicon d to a mountain b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                        ...                                                    question_text_truncated\n",
       "0  00002165364db923c7e6                        ...                          how did quebec nationalists see their province...\n",
       "1  000032939017120e6e44                        ...                          do you have an adopted dog, how would you enco...\n",
       "2  0000412ca6e4628ce2cf                        ...                          why does velocity affect time? does velocity a...\n",
       "3  000042bf85aa498cd78e                        ...                          how did otto von guericke used the magdeburg h...\n",
       "4  0000455dfa3e01eae3af                        ...                          can i convert montra helicon d to a mountain b...\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's lower all the words in our train and test sets\n",
    "train['question_text_truncated'] = train['question_text'].apply(lambda x: \" \".join([word.lower() for word in x.split()[:20]]))\n",
    "test['question_text_truncated'] = test['question_text'].apply(lambda x: \" \".join([word.lower() for word in x.split()[:20]]))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all the questions\n",
    "list_questions = list(train['question_text_truncated']) + list(test['question_text_truncated'])\n",
    "\n",
    "# Split the questions into words then join them all together and finally we remove duplicates\n",
    "unique_words = set((\" \".join(list_questions)).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give an index to each word staring from 2.\n",
    "index_from = 2\n",
    "\n",
    "# Making the vocabulary\n",
    "vocabulary = {k: (v + index_from) for v, k in enumerate(unique_words)}\n",
    "\n",
    "vocabulary[\"<PAD>\"] = 0\n",
    "vocabulary[\"<START>\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reversally': 2,\n",
       " 'reify': 3,\n",
       " 'lrc': 4,\n",
       " 'question.what': 5,\n",
       " 'megaton': 6,\n",
       " 'vybezapp.com,': 7,\n",
       " \"'ma'?\": 8,\n",
       " 'quora.com.': 9,\n",
       " 'agancy?': 10,\n",
       " 'knowble': 11,\n",
       " 'cologne': 12,\n",
       " 'icpc?': 13,\n",
       " '(ground-based': 14,\n",
       " 'polymerization': 15,\n",
       " 'ironies': 16,\n",
       " 'n=1,2,3': 17,\n",
       " 'speculations,': 18,\n",
       " 'galanus?': 19,\n",
       " 'pixels)': 20,\n",
       " 'refugees/jews': 21,\n",
       " 'syeed?': 22,\n",
       " 'whi': 23,\n",
       " '9500aed)': 24,\n",
       " 'legionnaire': 25,\n",
       " 'opinion-on’s': 26,\n",
       " 'levipil': 27,\n",
       " 'bleeding\"?': 28,\n",
       " 'blenkinsopp?': 29,\n",
       " 'nehruvian': 30,\n",
       " '\"jonah': 31,\n",
       " 'certified,': 32,\n",
       " 'guild,': 33,\n",
       " 'unknown?': 34,\n",
       " 'jsminingstatics?': 35,\n",
       " \"bialik's\": 36,\n",
       " 'cockapoo,': 37,\n",
       " 'dh51': 38,\n",
       " 'carcharodontosaurids': 39,\n",
       " 'misses': 40,\n",
       " '\"guy': 41,\n",
       " 'diplomacy\"': 42,\n",
       " 'film/tv?': 43,\n",
       " 'purohit,': 44,\n",
       " 'wall,”': 45,\n",
       " 'prescribed.': 46,\n",
       " 'ciroc': 47,\n",
       " 'objective:': 48,\n",
       " 'auperwel': 49,\n",
       " 'mendeley': 50,\n",
       " 'youngters': 51,\n",
       " 'nobeoka,': 52,\n",
       " '1/x=5?': 53,\n",
       " 'qota': 54,\n",
       " 'patel)': 55,\n",
       " '(fullmetal': 56,\n",
       " 'labbett': 57,\n",
       " 'doxycycline?': 58,\n",
       " 'satyantan': 59,\n",
       " 'tall/short,': 60,\n",
       " 'pubmed?': 61,\n",
       " '\\\\dfrac{a(θ)}{b(θ)}': 62,\n",
       " '7.1': 63,\n",
       " 'guardians?': 64,\n",
       " 'spankophile?': 65,\n",
       " ',16gb': 66,\n",
       " 'biomechanics?': 67,\n",
       " 'hp.': 68,\n",
       " 'offset': 69,\n",
       " 'cocoa/cacao?': 70,\n",
       " 'selsun': 71,\n",
       " 'espino': 72,\n",
       " 'feed)?': 73,\n",
       " 'diggory': 74,\n",
       " 'thrush?': 75,\n",
       " 'fullcontact/clearbit': 76,\n",
       " 'schertle?': 77,\n",
       " 'tolle’s': 78,\n",
       " 'iker': 79,\n",
       " 'marsupials?': 80,\n",
       " 'pierce': 81,\n",
       " 'raftel': 82,\n",
       " 'elhamdullillah?': 83,\n",
       " 'serc.': 84,\n",
       " 'doubters,': 85,\n",
       " 'rome’s': 86,\n",
       " 'patel': 87,\n",
       " 'x^1/x=4?': 88,\n",
       " '[math]10^24[/math]': 89,\n",
       " 'nginx,': 90,\n",
       " 'chopra': 91,\n",
       " 'you.': 92,\n",
       " 'relevant?': 93,\n",
       " 'loic?': 94,\n",
       " 'terms),': 95,\n",
       " '1.0': 96,\n",
       " '123/315?': 97,\n",
       " 'rusted?': 98,\n",
       " 'booty': 99,\n",
       " 'sociologist?': 100,\n",
       " 'rafidha': 101,\n",
       " 'convicts': 102,\n",
       " 'atmananda': 103,\n",
       " 'cm/pm?': 104,\n",
       " 'guards.': 105,\n",
       " 'feely': 106,\n",
       " \"leadership's\": 107,\n",
       " 'patch.com': 108,\n",
       " 'tasting?': 109,\n",
       " '‘fate’': 110,\n",
       " '[little': 111,\n",
       " 'june.': 112,\n",
       " 'lexicon': 113,\n",
       " 'på': 114,\n",
       " 'acct.': 115,\n",
       " 'ohaneze': 116,\n",
       " 'uglyness?': 117,\n",
       " 'procesor': 118,\n",
       " 'airbag?': 119,\n",
       " 'provode': 120,\n",
       " '\"occam\\'s': 121,\n",
       " 'wepsite?': 122,\n",
       " 'ayotte': 123,\n",
       " 'abandonment?': 124,\n",
       " 'screemfest?': 125,\n",
       " 'da': 126,\n",
       " 'luft': 127,\n",
       " 'landfills?': 128,\n",
       " '\"bigger\"': 129,\n",
       " 'buttons,': 130,\n",
       " 'gravitional': 131,\n",
       " 'hasidus?': 132,\n",
       " 'prefectures': 133,\n",
       " 'smoking)?': 134,\n",
       " 'outdated,': 135,\n",
       " 'straitjackets': 136,\n",
       " 'highschoold': 137,\n",
       " \"a's,\": 138,\n",
       " 'kenya.': 139,\n",
       " 'ex-wife,': 140,\n",
       " 'safety(she)': 141,\n",
       " \"'heartbreak'?\": 142,\n",
       " 'pump)': 143,\n",
       " 'fools\"': 144,\n",
       " 'immigrants.': 145,\n",
       " 'scalminds': 146,\n",
       " 'repression': 147,\n",
       " 'influential': 148,\n",
       " 'lures': 149,\n",
       " 'svetlana': 150,\n",
       " \"blackstone's\": 151,\n",
       " '(types': 152,\n",
       " 'refuse/disallow': 153,\n",
       " 'tranist': 154,\n",
       " \"scrolling'\": 155,\n",
       " 'lpu': 156,\n",
       " 'umpiring?': 157,\n",
       " 'chatham': 158,\n",
       " 'filmfare': 159,\n",
       " 'god-one': 160,\n",
       " 'bipolar)': 161,\n",
       " 'margarita': 162,\n",
       " 'hexahydrate': 163,\n",
       " 'transvaginal': 164,\n",
       " \"advocate's\": 165,\n",
       " 'overbuilt': 166,\n",
       " 'censoring': 167,\n",
       " 'midddle': 168,\n",
       " 'anaemia?': 169,\n",
       " 'serb?': 170,\n",
       " 'commerece': 171,\n",
       " \"tiberius'\": 172,\n",
       " 'harrestment': 173,\n",
       " 'yonkers,': 174,\n",
       " '\"lucas': 175,\n",
       " 'aayd': 176,\n",
       " 'spetznaz,': 177,\n",
       " 'permanently,': 178,\n",
       " '*i’m': 179,\n",
       " 'sibutramine': 180,\n",
       " 'algorith?': 181,\n",
       " '(dubbed)': 182,\n",
       " '312.': 183,\n",
       " 'disadvantages': 184,\n",
       " 'partners)': 185,\n",
       " 'urnq': 186,\n",
       " 'dvaita,': 187,\n",
       " 'deserts': 188,\n",
       " \"beiber's\": 189,\n",
       " 'y^r': 190,\n",
       " 'hungover?': 191,\n",
       " 'vargia': 192,\n",
       " 'destination:': 193,\n",
       " 'martialed': 194,\n",
       " '√x(x+1)': 195,\n",
       " 'embarassement': 196,\n",
       " '“eben': 197,\n",
       " 'khuzestan)': 198,\n",
       " 'bedheets,': 199,\n",
       " 'balck': 200,\n",
       " 'fathar': 201,\n",
       " 'discharge:': 202,\n",
       " 'interrobang': 203,\n",
       " 'outperforms': 204,\n",
       " 'kukkiwon?': 205,\n",
       " 'satanists': 206,\n",
       " 'diu,': 207,\n",
       " 'model/strategy?': 208,\n",
       " '4400?': 209,\n",
       " 'otherprogramming?': 210,\n",
       " 'nexflix': 211,\n",
       " 'hl7,': 212,\n",
       " 'accomdated': 213,\n",
       " 'omarosa': 214,\n",
       " 'diplomat/ambassador': 215,\n",
       " 'mentally)': 216,\n",
       " 'kolkey': 217,\n",
       " 'weak\"': 218,\n",
       " 'sadhna,': 219,\n",
       " 'kerchief?': 220,\n",
       " 'd-subminiature': 221,\n",
       " '\"knight': 222,\n",
       " 'manufatured?': 223,\n",
       " 'indianleaders': 224,\n",
       " 'merchadise?': 225,\n",
       " 'linkbuilding?': 226,\n",
       " \"emperor's\": 227,\n",
       " '(ragging/misbehavior)': 228,\n",
       " 'brag?': 229,\n",
       " 'acrylonitrile': 230,\n",
       " \"network's\": 231,\n",
       " 'hourglasses': 232,\n",
       " 'look-at-me': 233,\n",
       " 'tool/module': 234,\n",
       " 'paltrow': 235,\n",
       " 'g09?': 236,\n",
       " 'missionary.': 237,\n",
       " 'dusty,': 238,\n",
       " 'eying': 239,\n",
       " \"dhupia's\": 240,\n",
       " 'paramedical/medical': 241,\n",
       " 'germany?': 242,\n",
       " 'kpca?': 243,\n",
       " 'wiggler': 244,\n",
       " 'southworth': 245,\n",
       " '\"alien': 246,\n",
       " 'sturgis': 247,\n",
       " 'patient-communication': 248,\n",
       " 'dodo?': 249,\n",
       " 'coinside': 250,\n",
       " 'ccnaa$cccnp?': 251,\n",
       " 'bomp': 252,\n",
       " '\"den': 253,\n",
       " 'innocent”?': 254,\n",
       " 'polices': 255,\n",
       " 'e&': 256,\n",
       " 'occurred\"': 257,\n",
       " 'jbim,': 258,\n",
       " ',howz': 259,\n",
       " 'kya': 260,\n",
       " \"tokidoki's\": 261,\n",
       " 'danced': 262,\n",
       " 'fears,\"': 263,\n",
       " 'water?': 264,\n",
       " \"bradshaw's\": 265,\n",
       " 'vitusa': 266,\n",
       " 'ggc': 267,\n",
       " 'earthers:': 268,\n",
       " 'sweatshirt?': 269,\n",
       " '\"glare': 270,\n",
       " '(agriculture)': 271,\n",
       " 'adobeacrobat': 272,\n",
       " 'apollo‐cooper': 273,\n",
       " 'runs)': 274,\n",
       " 'grapholigists': 275,\n",
       " 'monotheism,': 276,\n",
       " '\"polunga\"': 277,\n",
       " 'fillup': 278,\n",
       " 'rpo': 279,\n",
       " '\"evgeny\"?': 280,\n",
       " 'radicalized?': 281,\n",
       " 'shultz': 282,\n",
       " 'holly\"': 283,\n",
       " 'friends/family/sos': 284,\n",
       " 'insignificant?': 285,\n",
       " '(annoyed': 286,\n",
       " 'barclays,': 287,\n",
       " 'khagar': 288,\n",
       " \"narcissist'\": 289,\n",
       " '$740,': 290,\n",
       " \"'blur'\": 291,\n",
       " 'l,': 292,\n",
       " 'yzf-r15?': 293,\n",
       " 'spotless': 294,\n",
       " '146marks': 295,\n",
       " 'kind/field': 296,\n",
       " 'main.edu': 297,\n",
       " 'littlebrother': 298,\n",
       " 'virgity': 299,\n",
       " 'comey,': 300,\n",
       " 'attention,': 301,\n",
       " '\"dynamic\"': 302,\n",
       " 'hex-packing': 303,\n",
       " '‘african': 304,\n",
       " \"well'\": 305,\n",
       " 'aap': 306,\n",
       " 'flint': 307,\n",
       " 'paulianity?': 308,\n",
       " 'britten?': 309,\n",
       " 'orderliness,': 310,\n",
       " '(ex-slaveowners)': 311,\n",
       " 'steroids': 312,\n",
       " 'c9020-562': 313,\n",
       " 'french-style': 314,\n",
       " '(etp)?': 315,\n",
       " 'abeach?': 316,\n",
       " '(.kmz)': 317,\n",
       " 'enuma': 318,\n",
       " '309?': 319,\n",
       " 'war/conflict': 320,\n",
       " 'pvs': 321,\n",
       " 'tenpenny': 322,\n",
       " 'suboxone': 323,\n",
       " 'wall-mart?': 324,\n",
       " '[math]\\\\pi\\\\alpha\\\\pi^{-1}[/math]': 325,\n",
       " 'condomless': 326,\n",
       " 'posthumously': 327,\n",
       " 'refrigeration?': 328,\n",
       " 'simon’s': 329,\n",
       " 'revenge;': 330,\n",
       " 'feelonely': 331,\n",
       " 'xr9?': 332,\n",
       " 'colonialism)': 333,\n",
       " 'apply/test': 334,\n",
       " 'lacks)': 335,\n",
       " 'wrongly,': 336,\n",
       " 'tusk?': 337,\n",
       " 'tiffinbox4you.com': 338,\n",
       " 'downoad': 339,\n",
       " 'f9': 340,\n",
       " 'cohorts': 341,\n",
       " 'retroviruses,': 342,\n",
       " 'coffees,': 343,\n",
       " 'intervals.?': 344,\n",
       " 'or,maybe,': 345,\n",
       " 'kanda?': 346,\n",
       " 'mehak': 347,\n",
       " 'appart': 348,\n",
       " 'workex': 349,\n",
       " 'farts,': 350,\n",
       " 'ifrs9': 351,\n",
       " 'da\"?': 352,\n",
       " 'medicines/cosmetics?': 353,\n",
       " 'xbox)': 354,\n",
       " 'opkv?': 355,\n",
       " 'bangalore-mangalore': 356,\n",
       " 'skinned.': 357,\n",
       " 'emblems?': 358,\n",
       " 're-registration,': 359,\n",
       " 'companies.': 360,\n",
       " 'm0lecule': 361,\n",
       " 'robotics).': 362,\n",
       " 'wearables': 363,\n",
       " 'x->infinity': 364,\n",
       " 'allure?': 365,\n",
       " '\"split': 366,\n",
       " 'medicla': 367,\n",
       " 'iits/nits,': 368,\n",
       " 'bogies': 369,\n",
       " 'nagarhole': 370,\n",
       " 'gills?': 371,\n",
       " 'availlabl': 372,\n",
       " 'scrips': 373,\n",
       " 'mca?': 374,\n",
       " 'hayflick': 375,\n",
       " '(83)': 376,\n",
       " 'sons?': 377,\n",
       " 'hypotheticall': 378,\n",
       " 'nerdy-looking': 379,\n",
       " 'gbpeur?': 380,\n",
       " 'curriculums?': 381,\n",
       " 'overgrazing?': 382,\n",
       " 'kaopectate': 383,\n",
       " 'piscine?': 384,\n",
       " '“verified”': 385,\n",
       " 'turbofan': 386,\n",
       " 'up/down?': 387,\n",
       " \"hao'\": 388,\n",
       " 'krishnadevaraya': 389,\n",
       " 'discrepancy)?': 390,\n",
       " \"(do's\": 391,\n",
       " 'ten,': 392,\n",
       " 'bertil': 393,\n",
       " \"'tree\": 394,\n",
       " 'zalando': 395,\n",
       " '90-130': 396,\n",
       " 'exertso': 397,\n",
       " 'forget': 398,\n",
       " 'kiul?': 399,\n",
       " 'carding,': 400,\n",
       " 'taxation': 401,\n",
       " 'parade,': 402,\n",
       " 'eckovation?': 403,\n",
       " '(perfume)': 404,\n",
       " 'managers:': 405,\n",
       " 'ps3.': 406,\n",
       " 'pmgm': 407,\n",
       " 'shiawase': 408,\n",
       " 'preconditionfailure': 409,\n",
       " '(50-70+)': 410,\n",
       " '30tees': 411,\n",
       " 'regressives': 412,\n",
       " 'uncertain': 413,\n",
       " 'predicted,': 414,\n",
       " '(asu)': 415,\n",
       " 'packets,': 416,\n",
       " 'murcdoch': 417,\n",
       " 'intrinsic,': 418,\n",
       " 'reopened?': 419,\n",
       " 'nestea': 420,\n",
       " '(breast': 421,\n",
       " 'radiata?': 422,\n",
       " 'redmi4': 423,\n",
       " '6-500\"': 424,\n",
       " 'tiquets?': 425,\n",
       " \"douglas's\": 426,\n",
       " '(pre-registration)': 427,\n",
       " 'pyrroloquinoline': 428,\n",
       " 'frolic': 429,\n",
       " 'gaekwad': 430,\n",
       " 'sdks': 431,\n",
       " 'genuises': 432,\n",
       " '\"been': 433,\n",
       " 'dermatillomania?': 434,\n",
       " 'numismatica': 435,\n",
       " 'waiting': 436,\n",
       " 'j.s.': 437,\n",
       " 'cellbroadcast': 438,\n",
       " \"taboo',in\": 439,\n",
       " 'nativization': 440,\n",
       " '99designs': 441,\n",
       " 'markets/investing/trading': 442,\n",
       " 'peppy': 443,\n",
       " 'molly': 444,\n",
       " 'lowlands?': 445,\n",
       " 'chetniks': 446,\n",
       " 'stop/minimize': 447,\n",
       " 'erdogan': 448,\n",
       " 'trainman': 449,\n",
       " 'eyesite': 450,\n",
       " '579': 451,\n",
       " '\"unintentionally': 452,\n",
       " 'immobile': 453,\n",
       " 'enbrel': 454,\n",
       " '7rex': 455,\n",
       " 'nagelsmann': 456,\n",
       " 'englishmen': 457,\n",
       " 'lachit': 458,\n",
       " 'ramrodded': 459,\n",
       " 'literalist': 460,\n",
       " '\"fill': 461,\n",
       " 'evidence.”': 462,\n",
       " \"180's\": 463,\n",
       " '3.35': 464,\n",
       " 'zonda': 465,\n",
       " 'hostorical': 466,\n",
       " 'lsat,': 467,\n",
       " 'çakıroğlu': 468,\n",
       " 'amea?': 469,\n",
       " 'fraternizing': 470,\n",
       " 'lifeproof': 471,\n",
       " '(caanda)': 472,\n",
       " \"y'all\": 473,\n",
       " '(virus,': 474,\n",
       " 'weeeeelp': 475,\n",
       " 'osmf?': 476,\n",
       " 'h.pylori,': 477,\n",
       " 'presume?': 478,\n",
       " 'scatterplots?': 479,\n",
       " 'idepad': 480,\n",
       " '(alayhis-salam)?': 481,\n",
       " 'anus,': 482,\n",
       " 'hypsc?': 483,\n",
       " '45246': 484,\n",
       " '10-3': 485,\n",
       " 'oppenheim': 486,\n",
       " 'ph.d/m.s.': 487,\n",
       " 'aftetnoon': 488,\n",
       " 'underage': 489,\n",
       " 'listverse': 490,\n",
       " 'eggs)': 491,\n",
       " 'e^2x/e^2x-2?': 492,\n",
       " 'gastroscopy': 493,\n",
       " 'paganini': 494,\n",
       " 'justifiably': 495,\n",
       " 'acoustician': 496,\n",
       " 'composer,': 497,\n",
       " 'recives': 498,\n",
       " 'superimposed': 499,\n",
       " 'limits?': 500,\n",
       " 'wild)': 501,\n",
       " 'fu**in': 502,\n",
       " 'email?': 503,\n",
       " 'express?': 504,\n",
       " 'meaning/purpose,': 505,\n",
       " '\"mathophobe\"': 506,\n",
       " 'intercorse': 507,\n",
       " 'maurizio': 508,\n",
       " 'topless.': 509,\n",
       " 'schemata': 510,\n",
       " 'dalga': 511,\n",
       " 'executions': 512,\n",
       " '+tan4^(x)?': 513,\n",
       " 'cm².': 514,\n",
       " 'recruiters/interviewers': 515,\n",
       " 'antipathetic': 516,\n",
       " 'indie?': 517,\n",
       " 'naive?': 518,\n",
       " '\"河和湖\"': 519,\n",
       " 'hypotonia?': 520,\n",
       " 'w,': 521,\n",
       " 'child-friendly?': 522,\n",
       " 'left-leaning': 523,\n",
       " '\"callisto\"': 524,\n",
       " '336j–¹': 525,\n",
       " '(journal)?': 526,\n",
       " \"hilton's\": 527,\n",
       " 'mediatek?': 528,\n",
       " 'lazy…': 529,\n",
       " '^2]?': 530,\n",
       " 'invectives?': 531,\n",
       " '509': 532,\n",
       " 'cuso4?': 533,\n",
       " 'holier:': 534,\n",
       " 'spects': 535,\n",
       " 'paxil': 536,\n",
       " '(kirk)': 537,\n",
       " 'proficient': 538,\n",
       " 'raids': 539,\n",
       " 'usa/canada,': 540,\n",
       " 'gov’t': 541,\n",
       " 'residuary': 542,\n",
       " 'valueable': 543,\n",
       " 'baahubali,': 544,\n",
       " \"along'\": 545,\n",
       " '\"don\\'t\"?': 546,\n",
       " 'volkswagen,': 547,\n",
       " 'rums': 548,\n",
       " '(germany/ukraine)': 549,\n",
       " '(10-25)': 550,\n",
       " 'crox': 551,\n",
       " 'world’?': 552,\n",
       " 'tonsils,': 553,\n",
       " 'lifespan/death': 554,\n",
       " 'cordic': 555,\n",
       " 'pbcl4': 556,\n",
       " 'corral': 557,\n",
       " 'searchencrypt': 558,\n",
       " 'well-mannered,': 559,\n",
       " 'character\\u200b': 560,\n",
       " 'high-traffic': 561,\n",
       " 'ingos/aid,': 562,\n",
       " '(610/610)': 563,\n",
       " 'bbguns4less': 564,\n",
       " 'upsidc': 565,\n",
       " 'node*>?': 566,\n",
       " 'outside)': 567,\n",
       " 'formless?': 568,\n",
       " 'legs)?': 569,\n",
       " '2d,3h': 570,\n",
       " 'molavian': 571,\n",
       " '(one-sided)': 572,\n",
       " 'oct-1': 573,\n",
       " 'fret': 574,\n",
       " 'occasion?': 575,\n",
       " 'angstrom,': 576,\n",
       " 'transender?': 577,\n",
       " 'htc?': 578,\n",
       " 'derived/attributes': 579,\n",
       " 'blatt?': 580,\n",
       " 'food.\"?': 581,\n",
       " 'astro-chemistry': 582,\n",
       " 'pro-life?': 583,\n",
       " 'ktu': 584,\n",
       " 'consenting?': 585,\n",
       " \"bollywood's?\": 586,\n",
       " 'liberalist': 587,\n",
       " 'distroying': 588,\n",
       " 'boys/girls': 589,\n",
       " 'varese,': 590,\n",
       " '(5-9': 591,\n",
       " \"answer'\": 592,\n",
       " 'litcoin': 593,\n",
       " 'unprofessional': 594,\n",
       " '(deposit/credit).': 595,\n",
       " 'stripper?': 596,\n",
       " 'individualized': 597,\n",
       " 'dreamstime': 598,\n",
       " 'asskey': 599,\n",
       " 'complex!': 600,\n",
       " 'answer/reply': 601,\n",
       " 'clinic.': 602,\n",
       " 'interview.?': 603,\n",
       " 'slender.': 604,\n",
       " 'cognito': 605,\n",
       " '30mph,': 606,\n",
       " 'legalistic': 607,\n",
       " 'schools/immigration?': 608,\n",
       " '\"toby\"?': 609,\n",
       " 'l3?': 610,\n",
       " 'amrecian': 611,\n",
       " 'compatible,': 612,\n",
       " \"‘taboo'\": 613,\n",
       " 'neutrals': 614,\n",
       " 'gropers': 615,\n",
       " 'mayonnaise': 616,\n",
       " '(deluxe': 617,\n",
       " 'family-run': 618,\n",
       " 'unerasable': 619,\n",
       " 'forty-two': 620,\n",
       " 'applock': 621,\n",
       " 'togas': 622,\n",
       " 'qulification?': 623,\n",
       " 'infura?': 624,\n",
       " 'orthographic)?': 625,\n",
       " 'yury': 626,\n",
       " 'catrgory?': 627,\n",
       " 'flew': 628,\n",
       " 'hogtie': 629,\n",
       " \"dunyi's\": 630,\n",
       " '\"behind-the-scenes\"': 631,\n",
       " 'social/developmental': 632,\n",
       " 'yandere?': 633,\n",
       " '\"handles\"': 634,\n",
       " 'allowed': 635,\n",
       " 'nagpur/sambhalpur/sirmaur/jammu?': 636,\n",
       " 'government-subsidized?': 637,\n",
       " '\"ceaseless': 638,\n",
       " 'karaokes': 639,\n",
       " 'colloquial': 640,\n",
       " 'namemario?': 641,\n",
       " 'paged': 642,\n",
       " 'siamese': 643,\n",
       " 'techtonic': 644,\n",
       " 'luve': 645,\n",
       " 'terrier)': 646,\n",
       " \"study's\": 647,\n",
       " 'analyst/manger/cxo': 648,\n",
       " 'suffered\"': 649,\n",
       " 'one-way-view': 650,\n",
       " 'eurobasket': 651,\n",
       " 'rosa?': 652,\n",
       " 'mohanlal?': 653,\n",
       " 'vaporizer?': 654,\n",
       " 'ornithology': 655,\n",
       " 'flee,': 656,\n",
       " 'gandhi.\"': 657,\n",
       " 'ethiopians,': 658,\n",
       " 'guy/fiancé?': 659,\n",
       " 'yangon?': 660,\n",
       " 'designer/developer,': 661,\n",
       " 'eugenic': 662,\n",
       " 'uncommonly': 663,\n",
       " 'debacle,': 664,\n",
       " 'techinon': 665,\n",
       " 'lazarus': 666,\n",
       " 'wisconsin–river': 667,\n",
       " '9-to-5': 668,\n",
       " 'stars:': 669,\n",
       " 'cartoony': 670,\n",
       " '10°c?': 671,\n",
       " 'walloons?': 672,\n",
       " 'metamorphic': 673,\n",
       " 'icome': 674,\n",
       " '\"quivea\".': 675,\n",
       " 'ax51r2-4292bk': 676,\n",
       " 'anti-fascists': 677,\n",
       " 'frame-rate/exposure-time': 678,\n",
       " 'subjectsin': 679,\n",
       " '(7x^9)?': 680,\n",
       " 'encoder': 681,\n",
       " 'c=10;': 682,\n",
       " '\"arresting': 683,\n",
       " 'bratty/spoiled': 684,\n",
       " 'go!': 685,\n",
       " 'tires.': 686,\n",
       " 'aihm': 687,\n",
       " 'roma/sinti': 688,\n",
       " 'misdiagnose?': 689,\n",
       " 'gavel': 690,\n",
       " 'asimov': 691,\n",
       " 'pascle?': 692,\n",
       " 'sharp…': 693,\n",
       " '2/x=1/|_x_|': 694,\n",
       " 'epitome': 695,\n",
       " 'g/cm³': 696,\n",
       " '\"grudge': 697,\n",
       " 'jto(t)': 698,\n",
       " 'hnl?': 699,\n",
       " \"carlton's\": 700,\n",
       " 'tsmc?': 701,\n",
       " \"''for\": 702,\n",
       " 'typo3,': 703,\n",
       " '(a,ncr)': 704,\n",
       " 'interpolated': 705,\n",
       " 'algorithm..etc..)?': 706,\n",
       " \"taneyhill's\": 707,\n",
       " 'losers\"': 708,\n",
       " 'arrive?': 709,\n",
       " 'mike?': 710,\n",
       " 'god/goddess': 711,\n",
       " 'awesomely': 712,\n",
       " 'lightwood': 713,\n",
       " '29-year-old': 714,\n",
       " 'menu.': 715,\n",
       " 'india-so-called': 716,\n",
       " 'interrelationship': 717,\n",
       " 'solutioninn': 718,\n",
       " 'ulip?': 719,\n",
       " 'honouring': 720,\n",
       " 'is/means': 721,\n",
       " 'doggett': 722,\n",
       " 'ralley': 723,\n",
       " 'hermoine': 724,\n",
       " 'burning': 725,\n",
       " '[math](1+\\\\frac{1}{x})^x=x[/math]?': 726,\n",
       " '\"sherlock\"?': 727,\n",
       " 'keynesian?': 728,\n",
       " 'globalists?': 729,\n",
       " 'mgtow,': 730,\n",
       " 'choleric': 731,\n",
       " 'nantucket-cute\".': 732,\n",
       " 'canil?': 733,\n",
       " 'unsalted?': 734,\n",
       " 'schwinger': 735,\n",
       " 'micronesians?': 736,\n",
       " '2^3^2': 737,\n",
       " 'nexium?': 738,\n",
       " 'atttacted': 739,\n",
       " 'dandruff': 740,\n",
       " 'vagamon?': 741,\n",
       " 'paedophiles,': 742,\n",
       " 'buffett:': 743,\n",
       " 'leeco,': 744,\n",
       " \"netscape's\": 745,\n",
       " 'septum?': 746,\n",
       " 'battenberg?': 747,\n",
       " \"nepal's\": 748,\n",
       " 'fame/upvotes/follower': 749,\n",
       " 'treasure,': 750,\n",
       " 'eccleston)': 751,\n",
       " 'autograph?': 752,\n",
       " 'boffrand': 753,\n",
       " 'colleages': 754,\n",
       " \"dassler's\": 755,\n",
       " \"a's\": 756,\n",
       " 'freaks,': 757,\n",
       " '(bpd)': 758,\n",
       " 'shevhad': 759,\n",
       " '(iaas/paas)?': 760,\n",
       " '4cyl)?': 761,\n",
       " 'sbirs': 762,\n",
       " 'humbucker': 763,\n",
       " 'purushpur,': 764,\n",
       " 'fassbender': 765,\n",
       " 'optic?': 766,\n",
       " '‘australia’': 767,\n",
       " 'util.scanner': 768,\n",
       " 'completed/solved?': 769,\n",
       " \"bloomingdale's\": 770,\n",
       " 'beings.': 771,\n",
       " 'deity/god': 772,\n",
       " 'lubricate': 773,\n",
       " 'srot?': 774,\n",
       " 'huthies': 775,\n",
       " 'sbbj': 776,\n",
       " 'mcwg': 777,\n",
       " 'silence/stillness/emptiness/consciousness': 778,\n",
       " 'íntruments': 779,\n",
       " 'authors': 780,\n",
       " 'cluttered': 781,\n",
       " 'sonomama': 782,\n",
       " 'kheleo,': 783,\n",
       " \"savdhan'?\": 784,\n",
       " 'yamini': 785,\n",
       " 'cyclohexane.of': 786,\n",
       " \"'head\": 787,\n",
       " '‘welcome': 788,\n",
       " 'extorts': 789,\n",
       " 'parayana': 790,\n",
       " 'collectable?': 791,\n",
       " 'pant-suit': 792,\n",
       " 'preserve': 793,\n",
       " 'biejing': 794,\n",
       " 'lakh,': 795,\n",
       " '13th.': 796,\n",
       " 'vocoder?': 797,\n",
       " '35lacs': 798,\n",
       " '\"know\"?': 799,\n",
       " 'd2?': 800,\n",
       " 'y=a': 801,\n",
       " 'segmenting': 802,\n",
       " 'niemi': 803,\n",
       " 'hexo': 804,\n",
       " 'germaphobe?': 805,\n",
       " 'romania:': 806,\n",
       " 'alone:': 807,\n",
       " 'smelly,': 808,\n",
       " 'replant': 809,\n",
       " 'ropinirole': 810,\n",
       " 'clang,': 811,\n",
       " 'black-dominated': 812,\n",
       " 'jun-pyo': 813,\n",
       " 'anoxic': 814,\n",
       " '\\\\boxed{\\\\text{car': 815,\n",
       " 'aramide': 816,\n",
       " 'multi-point': 817,\n",
       " '99.990625%': 818,\n",
       " 'edifier': 819,\n",
       " 'profound,': 820,\n",
       " \"hegel's?\": 821,\n",
       " '2-dimensional': 822,\n",
       " 'genotype?': 823,\n",
       " 'cheekline': 824,\n",
       " 'rs.8': 825,\n",
       " 'egoistic,': 826,\n",
       " 'cleverness,': 827,\n",
       " 'bhagti': 828,\n",
       " 'audiobooks?': 829,\n",
       " '(cbo)': 830,\n",
       " 'won\"?': 831,\n",
       " 'hiit': 832,\n",
       " \"'smart\": 833,\n",
       " '+2-3+4-5+6-7+8-9+10-11+12-13+14-15+16-17+18-19+20?': 834,\n",
       " '“spine”': 835,\n",
       " 'dreamhost.': 836,\n",
       " '(rebirth)': 837,\n",
       " 'przhevalsky': 838,\n",
       " 'long/painful': 839,\n",
       " 'single-handed?': 840,\n",
       " 'nurture.': 841,\n",
       " 'bungler': 842,\n",
       " 'excursion,': 843,\n",
       " 'glick?': 844,\n",
       " 'catalytic': 845,\n",
       " '\"elts\"': 846,\n",
       " 'katy,': 847,\n",
       " 'mmo)': 848,\n",
       " \"weiner's\": 849,\n",
       " 'representatives.': 850,\n",
       " '¨do': 851,\n",
       " 'offending,': 852,\n",
       " 'danger:': 853,\n",
       " '(bisexual)': 854,\n",
       " 'pes,': 855,\n",
       " 'lucci': 856,\n",
       " 'bleach,': 857,\n",
       " 'characteristic\\u200b': 858,\n",
       " 'coton': 859,\n",
       " 'pansexual,': 860,\n",
       " 'throw\"': 861,\n",
       " 'hain\"?': 862,\n",
       " 'non-belief,': 863,\n",
       " 'tp': 864,\n",
       " 'kline?': 865,\n",
       " 'heels/with': 866,\n",
       " 'demobilisation?': 867,\n",
       " 'payroll,': 868,\n",
       " 'fledge': 869,\n",
       " 'isabela': 870,\n",
       " 'laliga?': 871,\n",
       " 'abstractions?': 872,\n",
       " '976': 873,\n",
       " 'stine': 874,\n",
       " 'cheeseburgers': 875,\n",
       " 'rejoiced': 876,\n",
       " 'behenji': 877,\n",
       " 'howard”': 878,\n",
       " 'straights,': 879,\n",
       " 'watership': 880,\n",
       " 'citizencjild': 881,\n",
       " 'consent': 882,\n",
       " 'sagamihara,': 883,\n",
       " '2/-9?': 884,\n",
       " 'ohanian': 885,\n",
       " '22912': 886,\n",
       " 'jestful?': 887,\n",
       " 'proofs\"?': 888,\n",
       " 'coffee!': 889,\n",
       " 'kiralik': 890,\n",
       " \"texas's\": 891,\n",
       " \"satisfied'\": 892,\n",
       " 'hanako-san': 893,\n",
       " '(grain)': 894,\n",
       " \"rashtra',\": 895,\n",
       " 'jetty': 896,\n",
       " '\"on\"?': 897,\n",
       " 'bobbie': 898,\n",
       " '(statistics?)': 899,\n",
       " 'autism\"?': 900,\n",
       " 'non-monotonic': 901,\n",
       " 'curlh?': 902,\n",
       " 'goldwing?': 903,\n",
       " 'wunderlist?': 904,\n",
       " '/drink': 905,\n",
       " 'one-million?': 906,\n",
       " 'data/coding': 907,\n",
       " 'emulsification': 908,\n",
       " 'stocks&shares': 909,\n",
       " '(non-eu)?': 910,\n",
       " 'hyphenated)?': 911,\n",
       " '263rd': 912,\n",
       " 'mha/mph': 913,\n",
       " '(slavery,': 914,\n",
       " 'abr': 915,\n",
       " 'cumuliform': 916,\n",
       " '200kwh?': 917,\n",
       " 'hated,': 918,\n",
       " 'erver': 919,\n",
       " 'transportation)': 920,\n",
       " 'learing?': 921,\n",
       " 'hextax': 922,\n",
       " 'maridhas': 923,\n",
       " 'coproducers,': 924,\n",
       " 'mturk?': 925,\n",
       " 'passive': 926,\n",
       " '[math]f(x)=x^x^x[/math]': 927,\n",
       " 'incapacitation?': 928,\n",
       " 'pf…etc?': 929,\n",
       " 'ukip?': 930,\n",
       " 'universe’s': 931,\n",
       " 'quotev,': 932,\n",
       " 'farns': 933,\n",
       " 'confederacy?': 934,\n",
       " 'soprano,': 935,\n",
       " 'age.explain': 936,\n",
       " '(accountant)': 937,\n",
       " 'epi': 938,\n",
       " '8-16mm': 939,\n",
       " 'antilles': 940,\n",
       " '-hillsboro': 941,\n",
       " 'achebe': 942,\n",
       " 'youngster,': 943,\n",
       " 'satyagrah?': 944,\n",
       " '$175k': 945,\n",
       " 'email-id': 946,\n",
       " 'incorrect?': 947,\n",
       " 'punch': 948,\n",
       " 'deactiavte': 949,\n",
       " 'f2m': 950,\n",
       " 'regester': 951,\n",
       " 'cosmetically': 952,\n",
       " 'sailormoon,': 953,\n",
       " '\"wings': 954,\n",
       " 'lump?': 955,\n",
       " '(duets': 956,\n",
       " 'jemelle': 957,\n",
       " 'volo': 958,\n",
       " 'houdini?': 959,\n",
       " 'softlock': 960,\n",
       " 'accident': 961,\n",
       " '$29,490': 962,\n",
       " '119.75?': 963,\n",
       " 'b€(0,1)?': 964,\n",
       " 'whitneys': 965,\n",
       " 'grief?': 966,\n",
       " '\"sartorial': 967,\n",
       " 'prepared?': 968,\n",
       " 'network/cyber': 969,\n",
       " 'imhotep': 970,\n",
       " 'cameltoes': 971,\n",
       " 'leth': 972,\n",
       " '(hot': 973,\n",
       " 'quiz.': 974,\n",
       " 'infraprojects': 975,\n",
       " 'puffy?': 976,\n",
       " 'bereftness': 977,\n",
       " '2x+3y=4': 978,\n",
       " 'codine': 979,\n",
       " 'murcielago': 980,\n",
       " \"windows's\": 981,\n",
       " 'reinvigorated': 982,\n",
       " 'gor?': 983,\n",
       " '3cnhw18nafu': 984,\n",
       " 'harass': 985,\n",
       " 'nationality),': 986,\n",
       " 'ashnarvy': 987,\n",
       " 'tanning': 988,\n",
       " 'resting?': 989,\n",
       " 'mat.': 990,\n",
       " 'debilitatingly': 991,\n",
       " 'females),': 992,\n",
       " 'vonnegut': 993,\n",
       " '\"experience\",': 994,\n",
       " 'reminders': 995,\n",
       " 'cryophytic': 996,\n",
       " 'aymaras,': 997,\n",
       " 'muse': 998,\n",
       " 'patna?': 999,\n",
       " 'unirt': 1000,\n",
       " 'numb,': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 497852\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary length:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our vocabulary, we will tokenize our sentences and pad with 0 the smaller ones to reach a 134 sequence size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>lenght_sentence</th>\n",
       "      <th>question_text_truncated</th>\n",
       "      <th>question_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>how did quebec nationalists see their province...</td>\n",
       "      <td>[307090, 274841, 75904, 134664, 188383, 268642...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>do you have an adopted dog, how would you enco...</td>\n",
       "      <td>[75807, 342458, 313501, 195397, 183101, 56765,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>why does velocity affect time? does velocity a...</td>\n",
       "      <td>[235819, 493802, 190879, 185030, 114694, 49380...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>how did otto von guericke used the magdeburg h...</td>\n",
       "      <td>[307090, 274841, 483046, 449130, 334021, 28286...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>can i convert montra helicon d to a mountain b...</td>\n",
       "      <td>[68698, 443904, 449839, 15094, 292060, 391766,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                        ...                                                         question_tokenized\n",
       "0  00002165364db923c7e6                        ...                          [307090, 274841, 75904, 134664, 188383, 268642...\n",
       "1  000032939017120e6e44                        ...                          [75807, 342458, 313501, 195397, 183101, 56765,...\n",
       "2  0000412ca6e4628ce2cf                        ...                          [235819, 493802, 190879, 185030, 114694, 49380...\n",
       "3  000042bf85aa498cd78e                        ...                          [307090, 274841, 483046, 449130, 334021, 28286...\n",
       "4  0000455dfa3e01eae3af                        ...                          [68698, 443904, 449839, 15094, 292060, 391766,...\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization of all words in a sentence using our vocabulary\n",
    "def sentence_tokenization(sentence, vocabulary):\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence.split():\n",
    "        tokenized_sentence.append(vocabulary[word])\n",
    "    return  tokenized_sentence\n",
    "    \n",
    "\n",
    "train[\"question_tokenized\"] = train[\"question_text_truncated\"].apply(lambda x: sentence_tokenization(x, vocabulary))\n",
    "test[\"question_tokenized\"] = test[\"question_text_truncated\"].apply(lambda x: sentence_tokenization(x, vocabulary))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_max_length = 20\n",
    "# 0 padding of the tokenized questions\n",
    "X = sequence.pad_sequences(train['question_tokenized'], maxlen = input_max_length, padding = \"post\", truncating= \"post\", value = 0)\n",
    "X_test = sequence.pad_sequences(test['question_tokenized'], maxlen = input_max_length, padding = \"post\", truncating= \"post\", value = 0)\n",
    "\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We prepare our data for the training and validation steps which we will make to avoid overfitting\n",
    "# Train/Validate split is less time consuming than several folds cross-validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelisation: Many-to-One model with LSTM layer and Embedding on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 150)           74677800  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               416768    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 75,094,825\n",
      "Trainable params: 75,094,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 150\n",
    "total_words = len(vocabulary) \n",
    "inputs_max_length = 20\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, embedding_vector_length, input_length = inputs_max_length))\n",
    "model.add(LSTM(units = 256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the Embedding task gathers about 99% of our total number of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must build a custom F1 metrics to plug it into our training steps with Keras\n",
    "def f1(y_true, y_pred):\n",
    "    '''\n",
    "    metric from here \n",
    "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "    '''\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model on our Train/Validate datasets\n",
    "\n",
    "#model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=[f1])\n",
    "#model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      " 315520/1306122 [======>.......................] - ETA: 14:14 - loss: 0.1373 - f1: 0.4264"
     ]
    }
   ],
   "source": [
    "# Training on the whole dataset\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=[f1])\n",
    "model.fit(X, y, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.where(model.predict(X_test, batch_size=1024) < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000163e3ea7c7a74cd7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002bd4fb5d505b9161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00007756b4a147d2b0b3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000086e4b7e1c7146103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000c4c3fbe8785a3090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  prediction\n",
       "0  0000163e3ea7c7a74cd7           1\n",
       "1  00002bd4fb5d505b9161           0\n",
       "2  00007756b4a147d2b0b3           0\n",
       "3  000086e4b7e1c7146103           0\n",
       "4  0000c4c3fbe8785a3090           0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions = pd.DataFrame({\"qid\":test[\"qid\"].values})\n",
    "predictions['prediction'] = pred_test\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('submission.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
